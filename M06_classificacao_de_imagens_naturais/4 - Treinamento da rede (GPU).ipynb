{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento da rede\n",
    "\n",
    "Este notebook aplica todos os conceitos que vimos em um treinamento completo para classificação de imagens. Todas as funções associadas com processamento do dataset foram incluídas no arquivo `dataset.py` e as funções que realizam o treinamento foram incluídas no arquivo `train.py`\n",
    "\n",
    "Este notebook leva algumas horas para rodar em uma GPU, e mais tempo ainda em uma CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula para baixar os dados. Execute apenas uma vez!\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "def download(root):\n",
    "\n",
    "    url_images = 'https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz'\n",
    "    url_targets = 'https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz'\n",
    "\n",
    "    download_and_extract_archive(url_images, root, remove_finished=False)\n",
    "    download_and_extract_archive(url_targets, root, remove_finished=False)\n",
    "\n",
    "#download('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/oxford_pets/annotations/list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Modifica a última camada do modelo para classificar em 2 classes\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m ds_train, ds_valid, logger \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Cada época demora em torno de 6.1 segundos em uma RTX3080 Ti\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/M06_classificacao_de_imagens_naturais/train.py:103\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, bs, num_epochs, lr, weight_decay, resize_size, seed, num_workers)\u001b[0m\n\u001b[1;32m    101\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Carrega o dataset\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m ds_train, ds_valid, class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/oxford_pets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Truque para testar o código, fingimos que o dataset possui menos imagens\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#ds_train.indices = ds_train.indices[:5*256]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/M06_classificacao_de_imagens_naturais/dataset.py:132\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(root, split, resize_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Ponderação das classes calculada em um dos notebooks\u001b[39;00m\n\u001b[1;32m    130\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.677\u001b[39m, \u001b[38;5;241m0.323\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mOxfordIIITPet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ds)\n\u001b[1;32m    134\u001b[0m n_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n\u001b[38;5;241m*\u001b[39msplit)\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/M06_classificacao_de_imagens_naturais/dataset.py:46\u001b[0m, in \u001b[0;36mOxfordIIITPet.__init__\u001b[0;34m(self, root, transforms)\u001b[0m\n\u001b[1;32m     44\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43manns_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m:   \u001b[38;5;66;03m# Remove comentários do arquivo\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         name, class_id, species_id, breed_id \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/oxford_pets/annotations/list.txt'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "import train\n",
    "\n",
    "params = {\n",
    "    'bs':256,\n",
    "    'num_epochs':300,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-2,\n",
    "    'resize_size':224,  # Tamanho das imagens de treinamento\n",
    "    'seed':0\n",
    "}\n",
    "model = models.resnet18()\n",
    "# Modifica a última camada do modelo para classificar em 2 classes\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "ds_train, ds_valid, logger = train.train(model, **params)\n",
    "# Cada época demora em torno de 6.1 segundos em uma RTX3080 Ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necessário, é possível continuar treinando a rede, mesmo após reiniciar o notebook. Bastaria fazermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/checkpoints/M06/checkpoint.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m50\u001b[39m,   \u001b[38;5;66;03m# Treina por mais 50 épocas\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Carrega pesos do modelo salvo\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/checkpoints/M06/checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18()\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/venv_vision/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/venv_vision/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Desktop/Foundations-of-Computer-Vision/venv_vision/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/checkpoints/M06/checkpoint.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import train\n",
    "\n",
    "\n",
    "params = {\n",
    "    'bs':256,\n",
    "    'num_epochs':50,   # Treina por mais 50 épocas\n",
    "    'lr':0.001,        # Learning rate menor do que o treinamento original\n",
    "    'weight_decay':1e-3,\n",
    "    'resize_size':224,  \n",
    "    'seed':1\n",
    "}\n",
    "\n",
    "# Carrega pesos do modelo salvo\n",
    "checkpoint = torch.load('../data/checkpoints/M06/checkpoint.pt')\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "# Treina mais um pouco. A linha está comentada para evitar a execução\n",
    "#ds_train, ds_valid, logger = train.train(model, **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
